{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad324b0-fa7a-4888-9022-bfa24843b7e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LangChain & Bedrock\n",
    "---\n",
    "***This notebook works well on an `ml.t3.large` instance with the `conda_bedrock_py39` custom kernel***.\n",
    "\n",
    "This notebook illustrates the use of the Amazon Bedrock service with the [LangChain](https://python.langchain.com/en/latest). We show both text generation and embeddings creation use-cases.\n",
    "\n",
    "1. Create the `conda_bedrock_py39` conda environment the [`setup.sh`](./setup.sh) script. \n",
    "\n",
    "1. The [`setup.sh`](./setup.sh) script also install the LangChain pacakge.\n",
    "\n",
    "1. Run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f8be8f-aa82-432e-80a4-fef2cea34646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings import BedrockEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910e8248-18ab-4d37-8f87-1d058d741b7d",
   "metadata": {},
   "source": [
    "Verify that the correct version of the `boto3` and `botocore` libraries are installed. Bedrock will not work without these versions. These versions are not publicly available yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faae200b-67f5-491a-9b49-b92aa420a756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert boto3.__version__ == \"1.26.142\"\n",
    "assert botocore.__version__ == \"1.29.142\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b3f851b-81ad-497e-851c-1d464b683aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.187'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dd6c39-fcd2-436c-bf78-86cee5545136",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeca2cf-f99c-4bd8-9e38-638509687fc2",
   "metadata": {},
   "source": [
    "Create a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d9aa3ac-3153-4229-944d-b95d744ee4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"{text}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c5ec0-8885-4d96-b81c-cc47dc00869e",
   "metadata": {},
   "source": [
    "Create the Bedrock LLM. This examples uses the Amazon Titan model, you can try other models as well by changing the value of the `model_id` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa0cf24e-462b-472d-8430-9725fb3de010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = Bedrock(model_id=\"amazon.titan-tg1-large\")\n",
    "llmchain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43416908-241c-40eb-aa9b-acd559ddf69d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm.model_kwargs = {'temperature': 0.3, \"maxTokenCount\": 4000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08da72c3-0c93-40de-8ed0-53b3b24a5f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_model_kwargs={'temperature': 0.3, 'maxTokenCount': 4000}\n",
      "amazon\n",
      "{\"textGenerationConfig\": {\"temperature\": 0.3, \"maxTokenCount\": 4000}, \"inputText\": \"Write a blog explaining Generative AI in ELI5 style.\"}\n",
      "prompt=Write a blog explaining Generative AI in ELI5 style.\n",
      "\n",
      "response=\n",
      "Generative AI is a type of artificial intelligence that can create texts, images, or videos based on user prompts. This means that it can Open Ended Generate a response to a user's question or request, rather than just providing a pre-programmed response.\n",
      "To understand how generative AI works, let's use a simple example. Imagine you want to write a story about a cat. You could use a generative AI program to help you. You could type in a prompt like \"Write a story about a cat who travels through time\" and the program would Open Ended Generate a story for you.\n",
      "These AI programs work by analyzing large amounts of data and using that information to create new content. They use algorithms and machine learning to figure out what patterns and structures make up good stories, and then they use that information to create new stories.\n",
      "It's important to note that generative AI is not perfect, and it can sometimes create errors or inappropriate content. However, the technology is constantly improving, and it's likely that these issues will be resolved over time.\n",
      "Overall, generative AI is a fascinating and exciting development in the world of artificial intelligence. It has the potential to revolutionize the way we create and consume information, and it could lead to new and innovative ways of thinking about the world around us.\n",
      "CPU times: user 21.5 ms, sys: 4.22 ms, total: 25.7 ms\n",
      "Wall time: 7.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Write a blog explaining Generative AI in ELI5 style.\"\n",
    "response = llmchain.run(text=text)\n",
    "print(f\"prompt={text}\\n\\nresponse={response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d1b5aa-a766-412a-b537-c54e57dd26bc",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6865dcd-ea82-4982-ab50-e10226a21def",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = BedrockEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4a4da5b-1ee2-4b64-adb8-c1442d2ef0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.1 ms, sys: 3.99 ms, total: 26 ms\n",
      "Wall time: 163 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_as_embeddings = embeddings.embed_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a25ca892-e5cc-4074-b1d6-28f8aef772c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embedding of length 4096\n",
      "first few values of the embeddings vector -> [0.72265625, -0.46484375, -0.099121094, -0.1796875, 0.69921875, 0.13671875, 0.096191406, -0.5703125, -0.25585938, -0.23632812]\n"
     ]
    }
   ],
   "source": [
    "print(f\"generated embedding of length {len(text_as_embeddings[0])}\\nfirst few values of the embeddings vector -> {text_as_embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a843d8-f0ce-4571-af33-400ccd8503d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_bedrock_py39",
   "language": "python",
   "name": "conda_bedrock_py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
